{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on GenerativeModel in module google.generativeai.generative_models object:\n",
      "\n",
      "class GenerativeModel(builtins.object)\n",
      " |  GenerativeModel(model_name: 'str' = 'gemini-pro', safety_settings: 'safety_types.SafetySettingOptions | None' = None, generation_config: 'generation_types.GenerationConfigType | None' = None, tools: 'content_types.FunctionLibraryType | None' = None, tool_config: 'content_types.ToolConfigType | None' = None, system_instruction: 'content_types.ContentType | None' = None)\n",
      " |  \n",
      " |  The `genai.GenerativeModel` class wraps default parameters for calls to\n",
      " |  `GenerativeModel.generate_content`, `GenerativeModel.count_tokens`, and\n",
      " |  `GenerativeModel.start_chat`.\n",
      " |  \n",
      " |  This family of functionality is designed to support multi-turn conversations, and multimodal\n",
      " |  requests. What media-types are supported for input and output is model-dependant.\n",
      " |  \n",
      " |  >>> import google.generativeai as genai\n",
      " |  >>> import PIL.Image\n",
      " |  >>> genai.configure(api_key='YOUR_API_KEY')\n",
      " |  >>> model = genai.GenerativeModel('models/gemini-pro')\n",
      " |  >>> result = model.generate_content('Tell me a story about a magic backpack')\n",
      " |  >>> result.text\n",
      " |  \"In the quaint little town of Lakeside, there lived a young girl named Lily...\"\n",
      " |  \n",
      " |  Multimodal input:\n",
      " |  \n",
      " |  >>> model = genai.GenerativeModel('models/gemini-pro')\n",
      " |  >>> result = model.generate_content([\n",
      " |  ...     \"Give me a recipe for these:\", PIL.Image.open('scones.jpeg')])\n",
      " |  >>> result.text\n",
      " |  \"**Blueberry Scones** ...\"\n",
      " |  \n",
      " |  Multi-turn conversation:\n",
      " |  \n",
      " |  >>> chat = model.start_chat()\n",
      " |  >>> response = chat.send_message(\"Hi, I have some questions for you.\")\n",
      " |  >>> response.text\n",
      " |  \"Sure, I'll do my best to answer your questions...\"\n",
      " |  \n",
      " |  To list the compatible model names use:\n",
      " |  \n",
      " |  >>> for m in genai.list_models():\n",
      " |  ...     if 'generateContent' in m.supported_generation_methods:\n",
      " |  ...         print(m.name)\n",
      " |  \n",
      " |  Arguments:\n",
      " |       model_name: The name of the model to query. To list compatible models use\n",
      " |       safety_settings: Sets the default safety filters. This controls which content is blocked\n",
      " |           by the api before being returned.\n",
      " |       generation_config: A `genai.GenerationConfig` setting the default generation parameters to\n",
      " |           use.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model_name: 'str' = 'gemini-pro', safety_settings: 'safety_types.SafetySettingOptions | None' = None, generation_config: 'generation_types.GenerationConfigType | None' = None, tools: 'content_types.FunctionLibraryType | None' = None, tool_config: 'content_types.ToolConfigType | None' = None, system_instruction: 'content_types.ContentType | None' = None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__ = __str__(self)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  count_tokens(self, contents: 'content_types.ContentsType' = None, *, generation_config: 'generation_types.GenerationConfigType | None' = None, safety_settings: 'safety_types.SafetySettingOptions | None' = None, tools: 'content_types.FunctionLibraryType | None' = None, tool_config: 'content_types.ToolConfigType | None' = None, request_options: 'helper_types.RequestOptionsType | None' = None) -> 'protos.CountTokensResponse'\n",
      " |      # fmt: off\n",
      " |  \n",
      " |  async count_tokens_async(self, contents: 'content_types.ContentsType' = None, *, generation_config: 'generation_types.GenerationConfigType | None' = None, safety_settings: 'safety_types.SafetySettingOptions | None' = None, tools: 'content_types.FunctionLibraryType | None' = None, tool_config: 'content_types.ToolConfigType | None' = None, request_options: 'helper_types.RequestOptionsType | None' = None) -> 'protos.CountTokensResponse'\n",
      " |  \n",
      " |  generate_content(self, contents: 'content_types.ContentsType', *, generation_config: 'generation_types.GenerationConfigType | None' = None, safety_settings: 'safety_types.SafetySettingOptions | None' = None, stream: 'bool' = False, tools: 'content_types.FunctionLibraryType | None' = None, tool_config: 'content_types.ToolConfigType | None' = None, request_options: 'helper_types.RequestOptionsType | None' = None) -> 'generation_types.GenerateContentResponse'\n",
      " |      A multipurpose function to generate responses from the model.\n",
      " |      \n",
      " |      This `GenerativeModel.generate_content` method can handle multimodal input, and multi-turn\n",
      " |      conversations.\n",
      " |      \n",
      " |      >>> model = genai.GenerativeModel('models/gemini-pro')\n",
      " |      >>> response = model.generate_content('Tell me a story about a magic backpack')\n",
      " |      >>> response.text\n",
      " |      \n",
      " |      ### Streaming\n",
      " |      \n",
      " |      This method supports streaming with the `stream=True`. The result has the same type as the non streaming case,\n",
      " |      but you can iterate over the response chunks as they become available:\n",
      " |      \n",
      " |      >>> response = model.generate_content('Tell me a story about a magic backpack', stream=True)\n",
      " |      >>> for chunk in response:\n",
      " |      ...   print(chunk.text)\n",
      " |      \n",
      " |      ### Multi-turn\n",
      " |      \n",
      " |      This method supports multi-turn chats but is **stateless**: the entire conversation history needs to be sent with each\n",
      " |      request. This takes some manual management but gives you complete control:\n",
      " |      \n",
      " |      >>> messages = [{'role':'user', 'parts': ['hello']}]\n",
      " |      >>> response = model.generate_content(messages) # \"Hello, how can I help\"\n",
      " |      >>> messages.append(response.candidates[0].content)\n",
      " |      >>> messages.append({'role':'user', 'parts': ['How does quantum physics work?']})\n",
      " |      >>> response = model.generate_content(messages)\n",
      " |      \n",
      " |      For a simpler multi-turn interface see `GenerativeModel.start_chat`.\n",
      " |      \n",
      " |      ### Input type flexibility\n",
      " |      \n",
      " |      While the underlying API strictly expects a `list[protos.Content]` objects, this method\n",
      " |      will convert the user input into the correct type. The hierarchy of types that can be\n",
      " |      converted is below. Any of these objects can be passed as an equivalent `dict`.\n",
      " |      \n",
      " |      * `Iterable[protos.Content]`\n",
      " |      * `protos.Content`\n",
      " |      * `Iterable[protos.Part]`\n",
      " |      * `protos.Part`\n",
      " |      * `str`, `Image`, or `protos.Blob`\n",
      " |      \n",
      " |      In an `Iterable[protos.Content]` each `content` is a separate message.\n",
      " |      But note that an `Iterable[protos.Part]` is taken as the parts of a single message.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          contents: The contents serving as the model's prompt.\n",
      " |          generation_config: Overrides for the model's generation config.\n",
      " |          safety_settings: Overrides for the model's safety settings.\n",
      " |          stream: If True, yield response chunks as they are generated.\n",
      " |          tools: `protos.Tools` more info coming soon.\n",
      " |          request_options: Options for the request.\n",
      " |  \n",
      " |  async generate_content_async(self, contents: 'content_types.ContentsType', *, generation_config: 'generation_types.GenerationConfigType | None' = None, safety_settings: 'safety_types.SafetySettingOptions | None' = None, stream: 'bool' = False, tools: 'content_types.FunctionLibraryType | None' = None, tool_config: 'content_types.ToolConfigType | None' = None, request_options: 'helper_types.RequestOptionsType | None' = None) -> 'generation_types.AsyncGenerateContentResponse'\n",
      " |      The async version of `GenerativeModel.generate_content`.\n",
      " |  \n",
      " |  start_chat(self, *, history: 'Iterable[content_types.StrictContentType] | None' = None, enable_automatic_function_calling: 'bool' = False) -> 'ChatSession'\n",
      " |      Returns a `genai.ChatSession` attached to this model.\n",
      " |      \n",
      " |      >>> model = genai.GenerativeModel()\n",
      " |      >>> chat = model.start_chat(history=[...])\n",
      " |      >>> response = chat.send_message(\"Hello?\")\n",
      " |      \n",
      " |      Arguments:\n",
      " |          history: An iterable of `protos.Content` objects, or equivalents to initialize the session.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_cached_content(cached_content: 'str | caching.CachedContent', *, generation_config: 'generation_types.GenerationConfigType | None' = None, safety_settings: 'safety_types.SafetySettingOptions | None' = None) -> 'GenerativeModel'\n",
      " |      Creates a model with `cached_content` as model's context.\n",
      " |      \n",
      " |      Args:\n",
      " |          cached_content: context for the model.\n",
      " |          generation_config: Overrides for the model's generation config.\n",
      " |          safety_settings: Overrides for the model's safety settings.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `GenerativeModel` object with `cached_content` as its context.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  cached_content\n",
      " |  \n",
      " |  model_name\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key=API_KEY)\n",
    "model = genai.GenerativeModel(model_name='gemini-1.5-flash')\n",
    "print(help(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Hi there! How can I help you today?\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\"\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 1,\n",
       "        \"candidates_token_count\": 11,\n",
       "        \"total_token_count\": 12\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_content(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.generate_content('hi, how are you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am doing well, thank you for asking!  How are you today?\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
